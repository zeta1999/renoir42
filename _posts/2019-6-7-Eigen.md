---
layout: post
mathjax: true
title: An approximative, randomized 'eigen' algorithm
---

I am currently preparing some 'demo' teaching sessions around a few subjects:
- Multithreaded & lock-less C++
- (Kalman) filtering
- Trading strategy backtesting, the technical part, i.e. /not/ the strategies generation process per se
- likely, a few other subjects... later.

While looking at Kalman filter litterature, I had an extra look at so-called tuning methods.

Let us put some background.

In its linear, discrete time form, Kalman filters can be described by the following equations

$$
\begin{array}{lcl}
x_{t+1} &=& A x_t + w \\
y &=& C x_t + v  \\
\left( \begin{array}{c} w \\ v \end{array} \right) & \sim & 
N \left( 0, 
\left(
  \begin{array}{cc}
    Q_w & 0 \\
    0 & R_v
  \end{array}
  \right)
\right)
\end{array} 
$$

Now, the best case is when covariance matrices $ Q_w $ & $ R_v $ are known.
In my last real-world usage of filters (spot-vol dynamics of EQ implied vol surfaces, if you ask), due to the way I used data, 
this was not a problem as everything is 'observable' eventually (as in: can be measured, not the matrix rank observability).

But what do you do if you cannot really observe empirical errors? One scenario I can imagine is when you are using some sensor 
and, well, cross checking would require... uhh, the same sensor and it all depends on some physical calibration of your
embedded system and this is just a pain, at the very last... 

One possible method consists in estimating a maximum likelihood.
Without much ado, it can be shown we end up with a matrix optimization problem.

$$
\begin{array}{l}
\underset{R_w, Q_w}{min}{ ln \, det P_{R_w,Q_w} + Y' P^{-1}_{R_w, Q_w} Y } \\
s.t. Q_w, R_w \geq 0 \\
P_{R_w,Q_w} = \overset{N+K-1}{\underset{i=1}{\Sigma}} \mathbb{O}_i Q_w \mathbb{O}'_i
+ \overset{N}{\underset{j=1}{\Sigma}}\mathbb{I}_j R_v \mathbb{I}_j'
\end{array}
$$

One solution is to decompose $ P $, Cholesky or other. Note that $ P $ is quite sparse.
To this end, I want to suggest, at least as a demo, some randomized matrix algorithm to find the Eigen decomposition.
The little experience I have with generalized sparse matrix packages make it unsuitable as a first demo: too much code, and 
so how do you embed that much code in your embedded solution? Who will audit the code?

So let us expose this algorithm...

... TBC (partial post) ...


References
[Identification of Disturbance Covariances Using Maximum Likelihood Estimation, Megan Zagrobelny &al, 2014](https://pdfs.semanticscholar.org/3f25/e01aaa64f7d5744288926b89b42f25c847ad.pdf)
[Online Learning of Eigenvectors, Dan Garber & al, MLR 2015](http://proceedings.mlr.press/v37/garberb15-supp.pdf)

